# -*- coding: utf-8 -*-
"""project 1 COVID-19 Cases Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrBHeWm9SRUCLjb0Sc5apg_JAStcVZ2R

## **Pytorch**

### **Download data from google links**
"""

!nvidia-smi

cat /proc/cpuinfo

import psutil 
ram_gb = psutil.virtual_memory().total / 1e9
print(ram_gb)

training_path = "covid_training.csv"
testing_path = "covid_testing.csv"

!gdown --id "19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF" --output covid_training.csv
!gdown --id "1CE240jLm2npU-tdz81-oVKEF3T2yfT1O" --output covid_testing.csv

"""### **Import package**


"""

"""data preprocessing"""
import csv
import os
import pandas as pd 
import numpy as np
"""pytorch"""
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

"""visualization"""
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

"""fix pytorch framework seed"""
seed = 1214
torch.backends.cudnn.deterministic = True #fix algorithm avoid output different
torch.backends.cudnn.benchmark = False #do not need to spend extra time to search best convolution algorithm in each layer
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
  torch.cuda.manual_seed_all(seed) #fix all GPU seed

print(torch.__version__)

def train(tr_set, dv_set, model, config, device):

  n_epochs = config["n_epochs"]
  # Setup optimizer
  optimizer = getattr(torch.optim, config["optimizer"])(model.parameters(), **config['optim_hparas']) #getattr: get class attribute or function(could directly input parameters);  initialize the optimizer by registering the model?™s parameters that need to be trained, and passing in the learning rate hyperparameter.
  min_mse = 1000. # thershold value for loss
  loss_record = {"train":[], "dev":[]} # for recording training loss
  early_stop_cnt = 0 #early stop count
  epoch = 0
  while epoch < n_epochs:
    model.train() # set model to the train mode
    for x, y in tr_set: # iterate through the dataloader
      optimizer.zero_grad() # set gradient to zero
      x, y = x.to(device), y.to(device) # move data to device (cpu/cuda)
      pred = model(x) # forward pass (compute output)
      mse_loss = model.cal_loss(pred, y) # compute loss
      mse_loss.backward() # compute gradient (backpropagation)
      optimizer.step() #update parameter
      loss_record["train"].append(mse_loss.detach().cpu().item()) #block backpropagation, then put tensor back to cpu and get item value
    
    # After each epoch, test your model on the validation (development) set.
    dev_mse = dev(dv_set, model,device) #Validation function
    if dev_mse < min_mse:
      # Save model if your model improved
      min_mse = dev_mse
      print("saving model (epoch = {:4d}, loss = {:4f})".format(epoch +1, min_mse))
      torch.save(model.state_dict(), config["save_path"])
      early_stop_cnt = 0
    else:
      early_stop_cnt += 1
    epoch += 1
    loss_record['dev'].append(dev_mse)
    if early_stop_cnt > config["early_stop"]:
      # Stop training if your model stops improving for "config['early_stop']" epochs.
      break
  print('Finished training after {} epochs'.format(epoch))
  return min_mse, loss_record

"""#### **Validation**"""

def dev(dv_set, model, device):
  model.eval()
  total_loss = 0
  for x, y in dv_set: ## iterate through the dataloader
    x, y = x.to(device), y.to(device) # # move data to device (cpu/cuda)
    with torch.no_grad(): ## disable gradient calculation
      pred = model(x) # forward pass (compute output)
      mse_loss = model.cal_loss(pred, y) # compute loss
    total_loss += mse_loss.detach().cpu().item()*len(x) # accumulate loss
  total_loss = total_loss/len(dv_set.dataset)
  return total_loss

"""#### **testing**"""

def test(tt_test, model, device):
  model.eval()
  preds = []
  for x in tt_test:
    x = x.to(device)
    with torch.no_grad():
      pred = model(x)
      preds.append(pred.detach().cpu()) # collect prediction
  preds = torch.cat(preds, dim = 0).numpy() # concatenate all predictions and convert to a numpy array  
  return preds

"""# **Setup Hyper-parameters**

`config` contains hyper-parameters for training and the path to save your model.
"""

device = get_device()                 # get the current available device ('cpu' or 'cuda')
os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/
target_only = False

config = {
    'n_epochs': 3000,                # maximum number of epochs
    'batch_size': 270,               # mini-batch size for dataloader
    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.001,                 # learning rate of SGD
        'momentum': 0.9              # momentum for SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'save_path': 'models/model.pth'  # your model will be saved here
}
