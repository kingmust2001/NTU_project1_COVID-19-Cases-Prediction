# -*- coding: utf-8 -*-
"""project 1 COVID-19 Cases Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrBHeWm9SRUCLjb0Sc5apg_JAStcVZ2R

## **Pytorch**

### **Download data from google links**
"""

!nvidia-smi

cat /proc/cpuinfo

import psutil 
ram_gb = psutil.virtual_memory().total / 1e9
print(ram_gb)

training_path = "covid_training.csv"
testing_path = "covid_testing.csv"

!gdown --id "19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF" --output covid_training.csv
!gdown --id "1CE240jLm2npU-tdz81-oVKEF3T2yfT1O" --output covid_testing.csv

"""### **Import package**


"""

"""data preprocessing"""
import csv
import os
import pandas as pd 
import numpy as np
"""pytorch"""
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

"""visualization"""
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

"""fix pytorch framework seed"""
seed = 1214
torch.backends.cudnn.deterministic = True #fix algorithm avoid output different
torch.backends.cudnn.benchmark = False #do not need to spend extra time to search best convolution algorithm in each layer
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
  torch.cuda.manual_seed_all(seed) #fix all GPU seed

print(torch.__version__)

"""# **Some Utilities**

You do not need to modify this part.
"""

def get_device():
    ''' Get device (if GPU is available, use GPU) '''
    return 'cuda' if torch.cuda.is_available() else 'cpu'

def plot_learning_curve(loss_record, title=''):
    ''' Plot learning curve of your DNN (train & dev loss) '''
    total_steps = len(loss_record['train'])
    x_1 = range(total_steps)
    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]
    figure(figsize=(6, 4))
    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')
    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')
    plt.ylim(0.0, 5.)
    plt.xlabel('Training steps')
    plt.ylabel('MSE loss')
    plt.title('Learning curve of {}'.format(title))
    plt.legend()
    plt.show()


def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):
    ''' Plot prediction of your DNN '''
    if preds is None or targets is None:
        model.eval()
        preds, targets = [], []
        for x, y in dv_set:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                pred = model(x)
                preds.append(pred.detach().cpu())
                targets.append(y.detach().cpu())
        preds = torch.cat(preds, dim=0).numpy()
        targets = torch.cat(targets, dim=0).numpy()

    figure(figsize=(5, 5))
    plt.scatter(targets, preds, c='r', alpha=0.5)
    plt.plot([-0.2, lim], [-0.2, lim], c='b')
    plt.xlim(-0.2, lim)
    plt.ylim(-0.2, lim)
    plt.xlabel('ground truth value')
    plt.ylabel('predicted value')
    plt.title('Ground Truth v.s. Prediction')
    plt.show()

"""### **Data preprocessing**

* read `.csv` files
* extract features
* split `covid.train.csv` into train/dev sets
* normalize features
"""

"""dataset define data preprocessing"""
class Covid19_data(Dataset):
  def __init__(self, path, mode="train", target_only=False):
    self.mode = mode
    """read data in numpy arrays"""
    with open(path, "r") as file: 
      data = list(csv.reader(file)) #csv reader will return csv data in str
      data = np.array(data[1:])[:, 1:].astype(float) #without id and feature columns names
    if not target_only:
      feats = list(range(93))
    else:
      feats = list(range(40))
      feats.append(57)
      feats.append(75)
    
    if mode == "test":
      """testing data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))"""
      data = data[:, feats]
      self.data = torch.FloatTensor(data)
    else:
      """# Training data (train/dev sets)
        # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))"""
      target = data[:, -1]
      data = data[:, feats]
      """Splitting training data into train & dev sets"""
      if mode == "train":
        indices = [i for i in range(len(data)) if i%10 != 0]
      elif mode == "dev":
        indices = [i for i in range(len(data)) if i%10 == 0]
      """Convert data into PyTorch tensors"""
      self.data = torch.FloatTensor(data[indices])
      self.target = torch.FloatTensor(target[indices])
    """Normalize features (you may remove this part to see what will happen)"""
    self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True))/self.data[:, 40:].std(dim=0, keepdim=True) #reduce dim(column mean); keepdim: output keep tensor dim
    self.dim = self.data.shape[1]
    print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'.format(mode, len(self.data), self.dim))

  def __getitem__(self, index):
    """Returns one sample at a time"""
    if self.mode in ["train", "dev"]:
      return self.data[index], self.target[index]
    else:
      return self.data[index]
  def __len__(self):
    return len(self.data)

#"""A DataLoader loads data from a given Dataset into batches.""""

"""parameter in dataloader"""
"""
pin_memory (bool, optional) ??If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below.
num_workers (int, optional) ??how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)
drop_last (bool, optional) ??set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)
"""
def pre_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):
  dataset = Covid19_data(path, mode, target_only= target_only)
  dataloader = DataLoader(dataset, batch_size, shuffle=(mode=="True"), drop_last = False, num_workers=0, pin_memory=True)
  return dataloader

"""# **Load data and model**"""

tr_set = pre_dataloader(training_path, 'train', config['batch_size'], target_only=target_only)
dv_set = pre_dataloader(training_path, 'dev', config['batch_size'], target_only=target_only)
tt_set = pre_dataloader(testing_path, 'test', config['batch_size'], target_only=target_only)

model = Neural_covid19(tr_set.dataset.dim).to(device) # Construct model and move to device

tr_set.dataset.dim

print(Neural_covid19(tr_set.dataset.dim))

