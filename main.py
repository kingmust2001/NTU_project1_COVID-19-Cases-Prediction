# -*- coding: utf-8 -*-
"""project 1 COVID-19 Cases Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrBHeWm9SRUCLjb0Sc5apg_JAStcVZ2R

## **Pytorch**

### **Download data from google links**
"""

!nvidia-smi

cat /proc/cpuinfo

import psutil 
ram_gb = psutil.virtual_memory().total / 1e9
print(ram_gb)

training_path = "covid_training.csv"
testing_path = "covid_testing.csv"

!gdown --id "19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF" --output covid_training.csv
!gdown --id "1CE240jLm2npU-tdz81-oVKEF3T2yfT1O" --output covid_testing.csv

"""### **Import package**


"""

"""data preprocessing"""
import csv
import os
import pandas as pd 
import numpy as np
"""pytorch"""
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

"""visualization"""
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

"""fix pytorch framework seed"""
seed = 1214
torch.backends.cudnn.deterministic = True #fix algorithm avoid output different
torch.backends.cudnn.benchmark = False #do not need to spend extra time to search best convolution algorithm in each layer
np.random.seed(seed)
torch.manual_seed(seed)
if torch.cuda.is_available():
  torch.cuda.manual_seed_all(seed) #fix all GPU seed

print(torch.__version__)

"""# **Some Utilities**

You do not need to modify this part.
"""

def get_device():
    ''' Get device (if GPU is available, use GPU) '''
    return 'cuda' if torch.cuda.is_available() else 'cpu'

def plot_learning_curve(loss_record, title=''):
    ''' Plot learning curve of your DNN (train & dev loss) '''
    total_steps = len(loss_record['train'])
    x_1 = range(total_steps)
    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]
    figure(figsize=(6, 4))
    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')
    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')
    plt.ylim(0.0, 5.)
    plt.xlabel('Training steps')
    plt.ylabel('MSE loss')
    plt.title('Learning curve of {}'.format(title))
    plt.legend()
    plt.show()


def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):
    ''' Plot prediction of your DNN '''
    if preds is None or targets is None:
        model.eval()
        preds, targets = [], []
        for x, y in dv_set:
            x, y = x.to(device), y.to(device)
            with torch.no_grad():
                pred = model(x)
                preds.append(pred.detach().cpu())
                targets.append(y.detach().cpu())
        preds = torch.cat(preds, dim=0).numpy()
        targets = torch.cat(targets, dim=0).numpy()

    figure(figsize=(5, 5))
    plt.scatter(targets, preds, c='r', alpha=0.5)
    plt.plot([-0.2, lim], [-0.2, lim], c='b')
    plt.xlim(-0.2, lim)
    plt.ylim(-0.2, lim)
    plt.xlabel('ground truth value')
    plt.ylabel('predicted value')
    plt.title('Ground Truth v.s. Prediction')
    plt.show()

"""### **Data preprocessing**

* read `.csv` files
* extract features
* split `covid.train.csv` into train/dev sets
* normalize features
"""

"""dataset define data preprocessing"""
class Covid19_data(Dataset):
  def __init__(self, path, mode="train", target_only=False):
    self.mode = mode
    """read data in numpy arrays"""
    with open(path, "r") as file: 
      data = list(csv.reader(file)) #csv reader will return csv data in str
      data = np.array(data[1:])[:, 1:].astype(float) #without id and feature columns names
    if not target_only:
      feats = list(range(93))
    else:
      feats = list(range(40))
      feats.append(57)
      feats.append(75)
    
    if mode == "test":
      """testing data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))"""
      data = data[:, feats]
      self.data = torch.FloatTensor(data)
    else:
      """# Training data (train/dev sets)
        # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))"""
      target = data[:, -1]
      data = data[:, feats]
      """Splitting training data into train & dev sets"""
      if mode == "train":
        indices = [i for i in range(len(data)) if i%10 != 0]
      elif mode == "dev":
        indices = [i for i in range(len(data)) if i%10 == 0]
      """Convert data into PyTorch tensors"""
      self.data = torch.FloatTensor(data[indices])
      self.target = torch.FloatTensor(target[indices])
    """Normalize features (you may remove this part to see what will happen)"""
    self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True))/self.data[:, 40:].std(dim=0, keepdim=True) #reduce dim(column mean); keepdim: output keep tensor dim
    self.dim = self.data.shape[1]
    print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'.format(mode, len(self.data), self.dim))

  def __getitem__(self, index):
    """Returns one sample at a time"""
    if self.mode in ["train", "dev"]:
      return self.data[index], self.target[index]
    else:
      return self.data[index]
  def __len__(self):
    return len(self.data)

#"""A DataLoader loads data from a given Dataset into batches.""""

"""parameter in dataloader"""
"""
pin_memory (bool, optional) – If True, the data loader will copy Tensors into CUDA pinned memory before returning them. If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below.
num_workers (int, optional) – how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)
drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)
"""
def pre_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):
  dataset = Covid19_data(path, mode, target_only= target_only)
  dataloader = DataLoader(dataset, batch_size, shuffle=(mode=="True"), drop_last = False, num_workers=0, pin_memory=True)
  return dataloader

"""### **Deep Neural Network**

NeuralNet is an nn.Module designed for regression. **The DNN consists of 2 fully-connected layers with ReLU activation.** This module also included a function cal_loss for calculating loss.
"""

class Neural_covid19(nn.Module):
  def __init__(self, input_dim):
    super(Neural_covid19, self).__init__()

    self.net = nn.Sequential(
        nn.Linear(input_dim, 64),
        nn.ReLU(),
        nn.Linear(64, 1)
    )
    # Mean squared error loss
    self.criteria = nn.MSELoss(reduction="mean")  #reduction (string, optional) – Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.'mean': the sum of the output will be divided by the number of elements in the output

  def forward(self, x): #Compute output of your NN
  #''' Given input of size (batch_size x input_dim), compute output of the network '''
    return self.net(x).squeeze(1)
  
  def cal_loss(self, pred, target):
    return self.criteria(pred, target)

"""### **Train/Dev/Test**

#### **Training**
"""

def train(tr_set, dv_set, model, config, device):

  n_epochs = config["n_epochs"]
  # Setup optimizer
  optimizer = getattr(torch.optim, config["optimizer"])(model.parameters(), **config['optim_hparas']) #getattr: get class attribute or function(could directly input parameters);  initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.
  min_mse = 1000. # thershold value for loss
  loss_record = {"train":[], "dev":[]} # for recording training loss
  early_stop_cnt = 0 #early stop count
  epoch = 0
  while epoch < n_epochs:
    model.train() # set model to the train mode
    for x, y in tr_set: # iterate through the dataloader
      optimizer.zero_grad() # set gradient to zero
      x, y = x.to(device), y.to(device) # move data to device (cpu/cuda)
      pred = model(x) # forward pass (compute output)
      mse_loss = model.cal_loss(pred, y) # compute loss
      mse_loss.backward() # compute gradient (backpropagation)
      optimizer.step() #update parameter
      loss_record["train"].append(mse_loss.detach().cpu().item()) #block backpropagation, then put tensor back to cpu and get item value
    
    # After each epoch, test your model on the validation (development) set.
    dev_mse = dev(dv_set, model,device) #Validation function
    if dev_mse < min_mse:
      # Save model if your model improved
      min_mse = dev_mse
      print("saving model (epoch = {:4d}, loss = {:4f})".format(epoch +1, min_mse))
      torch.save(model.state_dict(), config["save_path"])
      early_stop_cnt = 0
    else:
      early_stop_cnt += 1
    epoch += 1
    loss_record['dev'].append(dev_mse)
    if early_stop_cnt > config["early_stop"]:
      # Stop training if your model stops improving for "config['early_stop']" epochs.
      break
  print('Finished training after {} epochs'.format(epoch))
  return min_mse, loss_record

"""#### **Validation**"""

def dev(dv_set, model, device):
  model.eval()
  total_loss = 0
  for x, y in dv_set: ## iterate through the dataloader
    x, y = x.to(device), y.to(device) # # move data to device (cpu/cuda)
    with torch.no_grad(): ## disable gradient calculation
      pred = model(x) # forward pass (compute output)
      mse_loss = model.cal_loss(pred, y) # compute loss
    total_loss += mse_loss.detach().cpu().item()*len(x) # accumulate loss
  total_loss = total_loss/len(dv_set.dataset)
  return total_loss

"""#### **testing**"""

def test(tt_test, model, device):
  model.eval()
  preds = []
  for x in tt_test:
    x = x.to(device)
    with torch.no_grad():
      pred = model(x)
      preds.append(pred.detach().cpu()) # collect prediction
  preds = torch.cat(preds, dim = 0).numpy() # concatenate all predictions and convert to a numpy array  
  return preds

"""# **Setup Hyper-parameters**

`config` contains hyper-parameters for training and the path to save your model.
"""

device = get_device()                 # get the current available device ('cpu' or 'cuda')
os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/
target_only = False

config = {
    'n_epochs': 3000,                # maximum number of epochs
    'batch_size': 270,               # mini-batch size for dataloader
    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.001,                 # learning rate of SGD
        'momentum': 0.9              # momentum for SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'save_path': 'models/model.pth'  # your model will be saved here
}

"""# **Load data and model**"""

tr_set = pre_dataloader(training_path, 'train', config['batch_size'], target_only=target_only)
dv_set = pre_dataloader(training_path, 'dev', config['batch_size'], target_only=target_only)
tt_set = pre_dataloader(testing_path, 'test', config['batch_size'], target_only=target_only)

model = Neural_covid19(tr_set.dataset.dim).to(device) # Construct model and move to device

tr_set.dataset.dim

print(Neural_covid19(tr_set.dataset.dim))

"""# **start training**"""

model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)

plot_learning_curve(model_loss_record, title='deep model')

del model #clear model para
model = Neural_covid19(tr_set.dataset.dim).to(device)
ckpt = torch.load(config['save_path'], map_location = "cpu") # Load your best model, turn gpu to cpu
model.load_state_dict(ckpt)
plot_pred(dv_set, model, device)  # Show prediction on the validation set

"""# **Testing**
The predictions of your model on testing set will be stored at `pred.csv`.
"""

def save_pred(preds, file):
  with open(file, "w") as fp:
    writer = csv.writer(fp) #build cvs writer
    writer.writerow(["id", "test_postive"])
    for i, p in enumerate(preds):
      writer.writerow([i, p])
preds = test(tt_set, model, device)
save_pred(preds, "prediction.csv")

"""# **Tensorflow**

## **import package**
"""

"""tensorflow package"""
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import utils
from tensorflow.keras import optimizers
from sklearn.model_selection import train_test_split
#fix tensorflow seed
seed = 1214
tf.random.set_seed(seed)
print(" keras version:", keras.__version__, "\n", "tensorflow version:", tf.__version__)

train_tf = pd.read_csv(training_path)
x = train_tf.iloc[:, 1:-1]
y = train_tf.iloc[:, -1]
test_tf = pd.read_csv(testing_path)
print("training data shape:", train_tf.shape, "\n", "testing data shape", test_tf.shape, "\n", "training data:", "\n", train_tf, "\n", "testing data:", "\n", test_tf, "\n", x)

x = x.to_numpy().astype("float32")
y = y.to_numpy().astype("float32")
test_tf = test_tf.iloc[:, 1:].to_numpy().astype("float32")
print("training data shape:", x.shape, "\n", "testing data shape", test_tf.shape, "\n", "training data:", "\n", x, "\n", "testing data:", "\n", test_tf)

x_train_tf, x_test_tf, y_train_tf, y_test_tf = train_test_split(x, y, test_size = 0.2, random_state = 42)
print("x_train_tf shpae:", x_train_tf.shape, "x_test_tf shape:", x_test_tf.shape, "y_train_tf:", y_train_tf.shape, "y_test_tf shape", y_test_tf.shape)

model = Sequential()
model.add(Dense(64, input_shape=(93,), activation='relu'))
model.add(Dense(1))
model.summary()

config_tf = {
    'n_epochs': 3000,                # maximum number of epochs
    'batch_size': 270,               # mini-batch size for dataloader
    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.001,                 # learning rate of SGD
        'momentum': 0.9              # momentum for SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'save_path': 'models/model.pth'  # your model will be saved here
}

opt = tf.keras.optimizers.SGD(**config["optim_hparas"]) # model optimizer
# compiling the model with CC loss and optimizer! Along with Acc. metrics!
model.compile(loss="mse",optimizer=opt,metrics=[tf.keras.metrics.MeanSquaredError()])
# we will do 50 sets of run and in epochs, 10 will be the batch size
num_epochs = 100
history = model.fit(x_train_tf, y_train_tf, epochs=num_epochs, validation_split=0.2, batch_size=270)

# 對訓練過程的準確度繪圖
import matplotlib.pyplot as plt
plt.plot(history.history['mean_squared_error'], 'r')
plt.plot(history.history['val_mean_squared_error'], 'g')

# 對訓練過程的損失函數繪圖
plt.plot(history.history['loss'], 'r')
plt.plot(history.history['val_loss'], 'g')
plt.legend(['loss', 'val_loss'], loc='upper right')
plt.show()

from matplotlib import pyplot as plt
epoch_nums = range(1,num_epochs+1)
training_loss = history.history["loss"]
validation_loss = history.history["val_loss"]
plt.plot(epoch_nums, training_loss)
plt.plot(epoch_nums, validation_loss)
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['training', 'validation'], loc='upper right')
plt.show()

#-------------------------------------------

with open(training_path, "r") as file:
  data_tf_torch = list(csv.reader(file))
data_tf_torch = np.array(data_tf_torch[1:])[:, 1:].astype(float)
data_tf_torch

print(data_tf_torch[0], len(data_tf_torch[0]))

data_tf_torch.astype("float32")

class Covid19_data_tf():
  def __init__(self, path, mode="train", target_only=False):
    self.mode = mode
    """read data in numpy arrays"""
    with open(path, "r") as file: 
      data = list(csv.reader(file)) #csv reader will return csv data in str
      data = np.array(data[1:])[:, 1:].astype(float) #without id and feature columns names
    if not target_only:
      feats = list(range(93))
    else:
      feats = list(range(40))
      feats.append(57)
      feats.append(75)
    
    if mode == "test":
      """testing data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))"""
      data = data[:, feats]
      self.data = data.astype("float32")
    else:
      """# Training data (train/dev sets)
        # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))"""
      target = data[:, -1]
      data = data[:, feats]
      """Splitting training data into train & dev sets"""
      if mode == "train":
        indices = [i for i in range(len(data)) if i%10 != 0]
      elif mode == "dev":
        indices = [i for i in range(len(data)) if i%10 == 0]
      """Convert data into PyTorch tensors"""
      self.data = data[indices].astype("float32")
      self.target = target[indices].astype("float32")
    """Normalize features (you may remove this part to see what will happen)"""
    self.data[:, 40:] = (self.data[:, 40:] - self.data[:, 40:].mean(axis=0))/self.data[:, 40:].std(axis=0) #reduce dim(column mean); keepdim: output keep tensor dim
  def return_data(self):
    return self.data, self.target

x_train_tf, y_train_tf = Covid19_data_tf(training_path, "train", target_only=False).return_data()
print("x train:", x_train_tf, "\n", "y train:", y_train_tf)

x_dev_tf, y_dev_tf = Covid19_data_tf(training_path, "dev", target_only=False).return_data()
print("x train:", x_dev_tf, "\n", "y train:", y_dev_tf)

print(x_train_tf.shape, y_train_tf.shape, x_dev_tf.shape, y_dev_tf.shape)

"""## **Deep Neural Network - tensorfow**

NeuralNet is an nn.Module designed for regression. **The DNN consists of 2 fully-connected layers with ReLU activation.** This module also included a function cal_loss for calculating loss.
"""

"""model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(tf.keras.layers.Input(shape=(93,))),                         
  tf.keras.layers.Dense(64, activation = "relu"),
  tf.keras.layers.Dense(1, activation = "softmax")
])"""

model = Sequential()
model.add(Dense(64, input_dim=93, activation='relu'))
model.add(Dense(1, input_dim=64,))
model.summary()

config_tf = {
    'n_epochs': 3000,                # maximum number of epochs
    'batch_size': 270,               # mini-batch size for dataloader
    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)
    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)
        'lr': 0.001,                 # learning rate of SGD
        'momentum': 0.9              # momentum for SGD
    },
    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)
    'save_path': 'models/model.pth'  # your model will be saved here
}

opt = tf.keras.optimizers.SGD(**config["optim_hparas"]) # model optimizer
# compiling the model with CC loss and optimizer! Along with Acc. metrics!
model.compile(loss="mse",optimizer=opt,metrics=[tf.keras.metrics.MeanSquaredError()])

# we will do 50 sets of run and in epochs, 10 will be the batch size
num_epochs = 100
history = model.fit(x_train_tf, y_train_tf, epochs=num_epochs, validation_split=0.2, batch_size = config_tf['batch_size'])

model.evaluate(x_dev_tf, y_dev_tf)

model.summary()

# 對訓練過程的準確度繪圖
import matplotlib.pyplot as plt
plt.plot(history.history['mean_squared_error'], 'r')
plt.plot(history.history['val_mean_squared_error'], 'g')

# 對訓練過程的損失函數繪圖
plt.plot(history.history['loss'], 'r')
plt.plot(history.history['val_loss'], 'g')